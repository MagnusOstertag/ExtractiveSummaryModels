{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximize and compare the accuracy of the following extractive summarization models: keyBERT, SciBERTSUM, MemSum\n",
    "\n",
    "Project work to complete the exam in [Text Mining by Prof. Gianluca Moro in the AC 2022/23 in Bologna](https://www.unibo.it/en/teaching/course-unit-catalogue/course-unit/2022/446610).\n",
    "\n",
    "The dataset is given and already divided into train, test and validation. The models have two targets to predict: relevant sentences and the relevant tokens. The accuracy on both targets should be maximized. The models to be used are:\n",
    "\n",
    "* [MemSum](https://github.com/nianlonggu/memsum)\n",
    "* [SciBERTSUM](https://github.com/atharsefid/SciBERTSUM)\n",
    "* [keyBERT](https://github.com/MaartenGr/KeyBERT)\n",
    "  * Simply the relevant sentences to be extracted will be those containing at least k relevant tokens, with k = 1, 2 or 3 that is an hyper parameter to be tuned."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project should be developed in colab with code commented in such a way that every step is understandable even without your verbal explanation in the discussion session."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task description\n",
    "\n",
    "A dataset of social media-like dialoge is given with a ground truth of the sentences and the tokens (of the sentences) most relevant.\n",
    "\n",
    "The task is now to use different models trained on summarization, adapt them to the dataset given and compare their performance on the tasks of\n",
    "\n",
    "* extractive summarization\n",
    "* keyword generation\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "* ROUGE-N for MemSum, keyBERT [see](https://towardsdatascience.com/the-ultimate-performance-metric-in-nlp-111df6c64460)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-step Episodic Markov decision process extractive SUMmarizer (MemSum)\n",
    "\n",
    "Based on reinforcement-learning episodic Markov decision processes, this extractive summarizer uses at each step\n",
    "\n",
    "1. the text context of the sentence\n",
    "2. the global text context (!)\n",
    "3. and the extraction history (!)\n",
    "\n",
    "While iteratively selecting further sentences to extract, it also auto-selects the stop state. It produces concise summaries with little redudancy.\n",
    "\n",
    "Even though that the model is lightweight, it has SOTA performance on long documents from PubMed, arXiv, and GovReport\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SciBERTSUM\n",
    "\n",
    "Extends BERTSUM by\n",
    "\n",
    "1. adding a section embedding layer to include section information in the sentence vector\n",
    "2. applying a sparse attention mechanism where each sentence will attend locally to nearby sentences and (randomly) to a small number of global sentences\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keyBERT\n",
    "\n",
    "Uses BERT embeddings and pretrained models. Then extracts keyword n-gram candidates and with cosine similarity and diversification selects the best keywords."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Model | Technology | performance | context | history | trained on\n",
    "---|---|---|---|---|---\n",
    "MemSum | Markov decision processes | xy | local, global | extraction | arXiv, PubMed, GovReport\n",
    "SciBERTSUM | BERT with attention | xy | local, global | no | articles with presentation slides as ground truth\n",
    "keyBERT | BERT embeddings with cosine similarity | xy | cosine similarity, diversification | in diversification | own"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected results\n",
    "\n",
    "* MemSum, SciBERTSUM should be very good for long texts\n",
    "* MemSum should generate short summaries compared to other models, less redundant\n",
    "* keyBERT should be fast"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "[Recommendations how to install packages](https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m venv env/ # only if you don't have a virtual environment\n",
    "!source env/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /usr/local/lib/python3.8/dist-packages\n",
      "sysconfig: /usr/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[33mWARNING: Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /usr/local/lib/python3.8/dist-packages\n",
      "sysconfig: /usr/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /usr/local/include/python3.8/UNKNOWN\n",
      "sysconfig: /usr/include/python3.8\u001b[0m\n",
      "\u001b[33mWARNING: Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /usr/local/bin\n",
      "sysconfig: /usr/bin\u001b[0m\n",
      "\u001b[33mWARNING: Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /usr/local\n",
      "sysconfig: /usr\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://download.pytorch.org/whl/cu116/torch_stable.html\n",
      "Requirement already satisfied: numpy in /home/work/.local/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.22.4)\n",
      "Requirement already satisfied: pandas in /home/work/.local/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.5.1)\n",
      "Requirement already satisfied: torchsummary in /home/work/.local/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (1.5.1)\n",
      "Requirement already satisfied: matplotlib in /home/work/.local/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (3.4.3)\n",
      "Requirement already satisfied: seaborn in /home/work/.local/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (0.11.2)\n",
      "Requirement already satisfied: tqdm in /home/work/.local/lib/python3.8/site-packages (from -r requirements.txt (line 14)) (4.60.0)\n",
      "Requirement already satisfied: rouge_score in /home/work/.local/lib/python3.8/site-packages (from -r requirements.txt (line 15)) (0.1.2)\n",
      "Requirement already satisfied: pyrouge in /home/work/.local/lib/python3.8/site-packages (from -r requirements.txt (line 16)) (0.1.3)\n",
      "Collecting keybert\n",
      "  Downloading keybert-0.7.0.tar.gz (21 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 2.8 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: sklearn in /home/work/.local/lib/python3.8/site-packages (from -r requirements.txt (line 21)) (0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/work/.local/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 3)) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/work/.local/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/work/.local/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 10)) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/work/.local/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 10)) (8.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/work/.local/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 10)) (0.10.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/work/.local/lib/python3.8/site-packages (from seaborn->-r requirements.txt (line 11)) (1.7.1)\n",
      "Requirement already satisfied: absl-py in /home/work/.local/lib/python3.8/site-packages (from rouge_score->-r requirements.txt (line 15)) (0.12.0)\n",
      "Requirement already satisfied: nltk in /home/work/.local/lib/python3.8/site-packages (from rouge_score->-r requirements.txt (line 15)) (3.8.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in /home/work/.local/lib/python3.8/site-packages (from keybert->-r requirements.txt (line 19)) (0.24.1)\n",
      "Collecting rich>=10.4.0\n",
      "  Downloading rich-13.3.1-py3-none-any.whl (239 kB)\n",
      "\u001b[K     |████████████████████████████████| 239 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /home/work/.local/lib/python3.8/site-packages (from sentence-transformers->-r requirements.txt (line 20)) (1.8.1)\n",
      "Requirement already satisfied: torchvision in /home/work/.local/lib/python3.8/site-packages (from sentence-transformers->-r requirements.txt (line 20)) (0.9.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 39.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
      "\u001b[K     |████████████████████████████████| 190 kB 41.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/work/.local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 20)) (3.4.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 20)) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/work/.local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 20)) (3.7.4.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/work/.local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 20)) (21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/work/.local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 20)) (5.4.1)\n",
      "Collecting markdown-it-py<3.0.0,>=2.1.0\n",
      "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 4.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pygments<3.0.0,>=2.14.0\n",
      "  Using cached Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/work/.local/lib/python3.8/site-packages (from scikit-learn>=0.22.2->keybert->-r requirements.txt (line 19)) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/work/.local/lib/python3.8/site-packages (from scikit-learn>=0.22.2->keybert->-r requirements.txt (line 19)) (1.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/work/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->-r requirements.txt (line 20)) (2022.10.31)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6 MB 44.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->rouge_score->-r requirements.txt (line 15)) (7.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 20)) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 20)) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/work/.local/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 20)) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/work/.local/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 20)) (2.10)\n",
      "Building wheels for collected packages: keybert, sentence-transformers\n",
      "  Building wheel for keybert (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keybert: filename=keybert-0.7.0-py3-none-any.whl size=23789 sha256=ff7e00bf978e8b119dd3c9d88e5756f1b48498e089a9a06422a55871b62acc87\n",
      "  Stored in directory: /home/work/.cache/pip/wheels/6c/bc/8b/a51bee77aec33895e6c8c236144b4cc10875659c4d2c80f070\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125918 sha256=a84ac439dc0e13e1e08324c97232f0febaf0dd1cf62bbf0eb581e3f9cda670a6\n",
      "  Stored in directory: /home/work/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
      "Successfully built keybert sentence-transformers\n",
      "Installing collected packages: typing-extensions, tokenizers, mdurl, huggingface-hub, transformers, sentencepiece, pygments, markdown-it-py, sentence-transformers, rich, keybert\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.7.4.3\n",
      "    Uninstalling typing-extensions-3.7.4.3:\n",
      "      Successfully uninstalled typing-extensions-3.7.4.3\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.10.0\n",
      "    Uninstalling Pygments-2.10.0:\n",
      "      Successfully uninstalled Pygments-2.10.0\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /home/work/.local/include/python3.8/UNKNOWN\n",
      "sysconfig: /home/work/.local/include/python3.8\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = True\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.4.1 requires numpy~=1.19.2, but you have numpy 1.22.4 which is incompatible.\n",
      "tensorflow 2.4.1 requires typing-extensions~=3.7.4, but you have typing-extensions 4.4.0 which is incompatible.\n",
      "spyder 5.0.1 requires jedi==0.17.2, but you have jedi 0.18.0 which is incompatible.\n",
      "spyder 5.0.1 requires parso==0.7.0, but you have parso 0.8.2 which is incompatible.\n",
      "arviz 0.11.2 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.4.0 which is incompatible.\u001b[0m\n",
      "Successfully installed huggingface-hub-0.12.0 keybert-0.7.0 markdown-it-py-2.1.0 mdurl-0.1.2 pygments-2.14.0 rich-13.3.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.26.0 typing-extensions-4.4.0\n",
      "\u001b[33mWARNING: You are using pip version 21.1; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import config \n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the extractive summarization models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MemSum\n",
    "\n",
    "The model expects:\n",
    "\n",
    "- [x] The training data is now stored in a .jsonl file that contains a list of json info, one line for one training instance. Each json (or dictonary) contains two keys:\n",
    "  1. \"text\": the value for which is a python list of sentences, this represents the document you want to summarize;\n",
    "  2. \"summary\": the value is also a list of sentences. If represent the ground-truth summary. Because the summary can contain multiple sentences, so we store them as a list.\n",
    "- [x] repeat for not-training corpus\n",
    "- [x] high-ROUGE episodes for the training set\n",
    "- [x] download glove into the `model/glove/` folder\n",
    "\n",
    "- [ ] we need to furthermore make a dataset with all the tokes instead of sentences as the basic thing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/work/Dokumente/Studium/SimTech_MSc/Erasmus/Lectures/Text_mining/project/ExtractiveSummaryModels/MemSum-015ddda'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('MemSum-015ddda')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the train corpus of length 14729 we have the keys dict_keys(['text', 'summary'])\n",
      "[' Amanda: I baked  cookies. Do you want some?', ' Jerry: Sure!', \" Amanda: I'll bring you tomorrow :-)\"]\n",
      "[' Amanda: I baked  cookies. Do you want some?', \" Amanda: I'll bring you tomorrow :-)\"]\n",
      "-----------------------------------\n",
      "\n",
      "For the val corpus of length 818 we have the keys dict_keys(['text', 'summary'])\n",
      "[' A: Hi Tom, are you busy tomorrow’s afternoon?', ' B: I’m pretty sure I am. What’s up?', ' A: Can you go with me to the animal shelter?.']\n",
      "[' A: Hi Tom, are you busy tomorrow’s afternoon?', ' A: Can you go with me to the animal shelter?.', ' A: I want to get a puppy for my son.']\n",
      "-----------------------------------\n",
      "\n",
      "For the test corpus of length 819 we have the keys dict_keys(['text', 'summary'])\n",
      "[\" Hannah: Hey, do you have Betty's number?\", ' Amanda: Lemme check', ' Hannah: <file_gif>']\n",
      "[\" Hannah: Hey, do you have Betty's number?\", \" Amanda: Sorry, can't find it.\", ' Amanda: Ask Larry']\n",
      "-----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preprocessing custom data\n",
    "def preprocessing_memsum_data_sentences(name_dataset):\n",
    "    with open(os.path.join(config.ENV_PATH, config.DATA_PATH, name_dataset + '.json')) as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    memsum_training = os.path.join(config.ENV_PATH, config.DATA_PATH, str('memsum_sentences_' + name_dataset + \".jsonl\"))\n",
    "    with open(memsum_training, 'w') as outfile:\n",
    "        for l in json_data:\n",
    "            dialog_temp = {}\n",
    "\n",
    "            # the dialogue is the text\n",
    "            text_temp = []\n",
    "            sentences = l['dialogue'].split('#')\n",
    "            for s in sentences:\n",
    "                # remove empty strings\n",
    "                # to normalize between train and test, we need to remove the empty end of the sentence\n",
    "                s = s.strip('\\n') # TODO: this accidently also strips stuff like ')'\n",
    "                if s == '':\n",
    "                    continue\n",
    "                else:\n",
    "                    if s[-1] == ' ': # TODO: this does not work as expected\n",
    "                        text_temp.append(s[:-1])\n",
    "                    else:\n",
    "                        text_temp.append(s)\n",
    "            dialog_temp['text'] = text_temp\n",
    "            \n",
    "            # the relevant dialogue is the summary\n",
    "            dialog_temp['summary'] = [s.strip('#') for s in l['relevant_dialogue']] # remove the # because it is also removed from the training\n",
    "\n",
    "            # # check if sth goes wrong\n",
    "            # for s in dialog_temp['summary']:\n",
    "            #     if s not in dialog_temp['text']:\n",
    "            #         print(f\"The summary of the sentence '{s}' is not in text {dialog_temp['text']}\")\n",
    "\n",
    "            # dialog_temp['finegrained_relevant_dialogue']: I don't think this model can work with this\n",
    "\n",
    "            outfile.write(json.dumps(dialog_temp)+'\\n')\n",
    "\n",
    "memsum_datasets = [config.name_trainset, config.name_valset, config.name_testset]\n",
    "\n",
    "! mkdir -p data/\n",
    "\n",
    "for d in memsum_datasets:\n",
    "    preprocessing_memsum_data_sentences(d)\n",
    "\n",
    "    # check if the preprocessing worked\n",
    "    corpus = [ json.loads(line) for line in open(os.path.join(config.ENV_PATH, config.DATA_PATH, str('memsum_sentences_' + d + \".jsonl\"))) ]\n",
    "    print(f\"For the {d} corpus of length {len(corpus)} we have the keys {corpus[0].keys()}\")\n",
    "    print(corpus[0][\"text\"][:3])\n",
    "    print(corpus[0][\"summary\"][:3])\n",
    "    print(\"-----------------------------------\\n\")\n",
    "\n",
    "    data_path = f\"../data/memsum_sentences_{d}.jsonl\"\n",
    "\n",
    "    ! cp $data_path data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14729/14729 [14:38<00:00, 16.77it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.data_preprocessing.MemSum.utils import greedy_extract\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_corpus = [ json.loads(line) for line in open(\"data/memsum_sentences_train.jsonl\") ]\n",
    "for data in tqdm(train_corpus):\n",
    "    high_rouge_episodes = greedy_extract(data[\"text\"], data[\"summary\"], beamsearch_size = 2)\n",
    "    indices_list = []\n",
    "    score_list  = []\n",
    "\n",
    "    for indices, score in high_rouge_episodes:\n",
    "        indices_list.append( indices )\n",
    "        score_list.append(score)\n",
    "\n",
    "    data[\"indices\"] = indices_list\n",
    "    data[\"score\"] = score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/memsum_sentences_train_labelled.jsonl\",\"w\") as f:\n",
    "    for data in train_corpus:\n",
    "        f.write(json.dumps(data) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SciBERTSum\n",
    "\n",
    "- [ ] what is the maximum length of the tokens in the text? If $>512$, then set `max_pos` to this value\n",
    "- [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "--2023-02-09 11:55:04--  https://nlp.stanford.edu/software/stanford-corenlp-latest.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-latest.zip [following]\n",
      "--2023-02-09 11:55:04--  https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-latest.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 506248615 (483M) [application/zip]\n",
      "Saving to: ‘stanford-corenlp-latest.zip’\n",
      "\n",
      "stanford-corenlp-la 100%[===================>] 482,80M  5,08MB/s    in 93s     \n",
      "\n",
      "2023-02-09 11:56:39 (5,17 MB/s) - ‘stanford-corenlp-latest.zip’ saved [506248615/506248615]\n",
      "\n",
      "Archive:  stanford-corenlp-latest.zip\n",
      "   creating: stanford-corenlp-4.5.0/\n",
      "  inflating: stanford-corenlp-4.5.0/pom-java-11.xml  \n",
      "  inflating: stanford-corenlp-4.5.0/corenlp.sh  \n",
      "  inflating: stanford-corenlp-4.5.0/javax.json-api-1.0-sources.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/Makefile  \n",
      "  inflating: stanford-corenlp-4.5.0/input.txt  \n",
      "  inflating: stanford-corenlp-4.5.0/ejml-core-0.39-sources.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/jaxb-api-2.4.0-b180830.0359.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/stanford-corenlp-4.5.0-javadoc.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/joda-time.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/jollyday-0.4.9-sources.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/LIBRARY-LICENSES  \n",
      "  inflating: stanford-corenlp-4.5.0/protobuf-java-3.19.2.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/StanfordCoreNlpDemo.java  \n",
      "  inflating: stanford-corenlp-4.5.0/stanford-corenlp-4.5.0.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/README.txt  \n",
      "  inflating: stanford-corenlp-4.5.0/build.xml  \n",
      "  inflating: stanford-corenlp-4.5.0/input.txt.xml  \n",
      "  inflating: stanford-corenlp-4.5.0/pom-java-17.xml  \n",
      "  inflating: stanford-corenlp-4.5.0/jollyday.jar  \n",
      "   creating: stanford-corenlp-4.5.0/sutime/\n",
      "  inflating: stanford-corenlp-4.5.0/sutime/english.holidays.sutime.txt  \n",
      "  inflating: stanford-corenlp-4.5.0/sutime/spanish.sutime.txt  \n",
      "  inflating: stanford-corenlp-4.5.0/sutime/english.sutime.txt  \n",
      "  inflating: stanford-corenlp-4.5.0/sutime/defs.sutime.txt  \n",
      "  inflating: stanford-corenlp-4.5.0/sutime/british.sutime.txt  \n",
      "  inflating: stanford-corenlp-4.5.0/istack-commons-runtime-3.0.7.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/xom-1.3.7-sources.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/stanford-corenlp-4.5.0-models.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/sample-project-pom.xml  \n",
      "  inflating: stanford-corenlp-4.5.0/xom.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/javax.json.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/slf4j-api.jar  \n",
      "   creating: stanford-corenlp-4.5.0/tokensregex/\n",
      "  inflating: stanford-corenlp-4.5.0/tokensregex/color.input.txt  \n",
      "  inflating: stanford-corenlp-4.5.0/tokensregex/retokenize.txt  \n",
      "  inflating: stanford-corenlp-4.5.0/tokensregex/color.rules.txt  \n",
      "  inflating: stanford-corenlp-4.5.0/tokensregex/color.properties  \n",
      "  inflating: stanford-corenlp-4.5.0/pom.xml  \n",
      "  inflating: stanford-corenlp-4.5.0/input.txt.out  \n",
      "  inflating: stanford-corenlp-4.5.0/slf4j-simple.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/RESOURCE-LICENSES  \n",
      "  inflating: stanford-corenlp-4.5.0/javax.activation-api-1.2.0.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/jaxb-impl-2.4.0-b180830.0438.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/CoreNLP-to-HTML.xsl  \n",
      "  inflating: stanford-corenlp-4.5.0/jaxb-api-2.4.0-b180830.0359-sources.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/ejml-simple-0.39.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/istack-commons-runtime-3.0.7-sources.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/LICENSE.txt  \n",
      "  inflating: stanford-corenlp-4.5.0/javax.activation-api-1.2.0-sources.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/stanford-corenlp-4.5.0-sources.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/jaxb-impl-2.4.0-b180830.0438-sources.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/ejml-core-0.39.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/ShiftReduceDemo.java  \n",
      "   creating: stanford-corenlp-4.5.0/patterns/\n",
      " extracting: stanford-corenlp-4.5.0/patterns/places.txt  \n",
      "  inflating: stanford-corenlp-4.5.0/patterns/goldnames.txt  \n",
      " extracting: stanford-corenlp-4.5.0/patterns/goldplaces.txt  \n",
      " extracting: stanford-corenlp-4.5.0/patterns/otherpeople.txt  \n",
      "  inflating: stanford-corenlp-4.5.0/patterns/stopwords.txt  \n",
      "  inflating: stanford-corenlp-4.5.0/patterns/example.properties  \n",
      "  inflating: stanford-corenlp-4.5.0/patterns/presidents.txt  \n",
      "  inflating: stanford-corenlp-4.5.0/patterns/names.txt  \n",
      "  inflating: stanford-corenlp-4.5.0/SemgrexDemo.java  \n",
      "  inflating: stanford-corenlp-4.5.0/ejml-ddense-0.39.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/StanfordDependenciesManual.pdf  \n",
      "  inflating: stanford-corenlp-4.5.0/ejml-ddense-0.39-sources.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/ejml-simple-0.39-sources.jar  \n",
      "  inflating: stanford-corenlp-4.5.0/joda-time-2.10.5-sources.jar  \n"
     ]
    }
   ],
   "source": [
    "# bash stuff\n",
    "import os\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "\n",
    "!wget \"https://nlp.stanford.edu/software/stanford-corenlp-latest.zip\"\n",
    "!unzip \"stanford-corenlp-latest.zip\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, I cannot download the pre-processed data because of a google drive error. [Try again later](https://drive.google.com/file/d/1xYHXYoQBa7DJVrq0ePly58ioq2EmmVG8/view)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keyBERT\n",
    "\n",
    "According to the documentation one should not do preprocessing.\n",
    "\n",
    "- [x] keyBERT wants the data as a list of documents\n",
    "- [x] `CountVectorizer` to remove stop words and specify the length of the keywords\n",
    "- [x] `sentence-transformers` to create high-quality embeddings\n",
    "- [x] calculate the cosine similarity between candidates and the document\n",
    "- [x] trade-off accuracy and diversity to better represent the whole document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: keybert\n",
      "Version: 0.7.0\n",
      "Summary: KeyBERT performs keyword extraction with state-of-the-art transformer models.\n",
      "Home-page: https://github.com/MaartenGr/keyBERT\n",
      "Author: Maarten Grootendorst\n",
      "Author-email: maartengrootendorst@gmail.com\n",
      "License: UNKNOWN\n",
      "Location: /home/work/.local/lib/python3.8/site-packages\n",
      "Requires: sentence-transformers, scikit-learn, rich, numpy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "! pip show keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# preprocessing custom data\n",
    "def preprocessing_keybert_data_tokens(name_dataset):\n",
    "    with open(os.path.join(config.ENV_PATH, config.DATA_PATH, name_dataset + '.json')) as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    keybert_docs, keybert_keywords = [], []\n",
    "\n",
    "    for l in json_data:\n",
    "        keybert_docs.append(l['dialogue'])\n",
    "        keybert_keywords.append(l['finegrained_relevant_dialogue'])\n",
    "\n",
    "    keybert_docs_path = os.path.join(config.ENV_PATH, config.DATA_PATH, str('keybert_token_docs_' + name_dataset + \".txt\"))\n",
    "    keybert_keywords_path = os.path.join(config.ENV_PATH, config.DATA_PATH, str('keybert_token_keywords_' + name_dataset + \".txt\"))\n",
    "    with open(keybert_docs_path, 'wb') as outfile_docs:\n",
    "        pickle.dump(keybert_docs, outfile_docs)\n",
    "    with open(keybert_keywords_path, 'wb') as outfile_keywords:\n",
    "        pickle.dump(keybert_keywords, outfile_keywords)\n",
    "\n",
    "keybert_datasets = [config.name_trainset, config.name_valset, config.name_testset]\n",
    "\n",
    "for d in keybert_datasets:\n",
    "    preprocessing_keybert_data_tokens(d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] token limit? then split the documents into paragraphs and use mean pooling for the resulting vector\n",
    "\n",
    "The resulting keywords are n-grams of different size. But if we get the range of the keywords we can directly use the parameter `keyphrase_ngram_range` to feed it into `keyBERT`. BUT as one can see from below, it is not really practical to have a $61$-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now it has a crack on it, my heel broke as I was leaving to work so i had to go back to change causing me to be late to work, some *idiot* spilt coffee all over my presentation notes and because Malcome is such a jackass, of course he thought that making copies was *optional* even though it's his job\n",
      "61 \n",
      "\n",
      "googled me out, because he thought that i had a crush on him, because 2 weeks earlier  I LIKED A POST ON FB announcing that he was going to accompany poets on the guitar during the poetry reading\n",
      "39 \n",
      "\n",
      "can even go and work in the petrol station till 10pm and pop home on the late train if I want to, saves me food money to have a day or two of home cooking, plus it's a lot nicer\n",
      "40 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(config.ENV_PATH, config.DATA_PATH, str('keybert_token_keywords_' + \"train\" + \".txt\")), 'rb') as f:\n",
    "    train_keywords = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(config.ENV_PATH, config.DATA_PATH, str('keybert_token_keywords_' + \"test\" + \".txt\")), 'rb') as f:\n",
    "    test_keywords = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(config.ENV_PATH, config.DATA_PATH, str('keybert_token_keywords_' + \"val\" + \".txt\")), 'rb') as f:\n",
    "    val_keywords = pickle.load(f)\n",
    "\n",
    "longest_ngram_train = 0\n",
    "longest_ngram_train_text = ''\n",
    "for i in train_keywords:\n",
    "    for j in i:\n",
    "        if len(j.split(' ')) > longest_ngram_train:\n",
    "            longest_ngram_train = len(j.split(' '))\n",
    "            longest_ngram_train_text = j\n",
    "print(longest_ngram_train_text)\n",
    "print(longest_ngram_train, '\\n')\n",
    "\n",
    "longest_ngram_test = 0\n",
    "longest_ngram_test_text = ''\n",
    "for i in test_keywords:\n",
    "    for j in i:\n",
    "        if len(j.split(' ')) > longest_ngram_test:\n",
    "            longest_ngram_test = len(j.split(' '))\n",
    "            longest_ngram_test_text = j\n",
    "print(longest_ngram_test_text)\n",
    "print(longest_ngram_test, '\\n')\n",
    "\n",
    "longest_ngram_val = 0\n",
    "longest_ngram_val_text = ''\n",
    "for i in val_keywords:\n",
    "    for j in i:\n",
    "        if len(j.split(' ')) > longest_ngram_val:\n",
    "            longest_ngram_val = len(j.split(' '))\n",
    "            longest_ngram_val_text = j\n",
    "print(longest_ngram_val_text)\n",
    "print(longest_ngram_val, '\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the [choices of bert models](https://www.sbert.net/docs/pretrained_models.html), the passage models (queries from bing -> relevant passages) make the most sense. To compare to the other publications, also the model trained on scientific citations can be used. The following models make sense:\n",
    "\n",
    "* `paraphrase-distilroberta-base-v2`: should perform well according to the docs\n",
    "* `msmarco`: passage ranking looks similar, here we use different versions\n",
    "* `specter`: trained on scientific citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)2b9e5/.gitattributes: 100%|██████████| 736/736 [00:00<00:00, 167kB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 52.4kB/s]\n",
      "Downloading (…)3c1ed2b9e5/README.md: 100%|██████████| 3.74k/3.74k [00:00<00:00, 827kB/s]\n",
      "Downloading (…)1ed2b9e5/config.json: 100%|██████████| 686/686 [00:00<00:00, 150kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 122/122 [00:00<00:00, 27.3kB/s]\n",
      "Downloading (…)c1ed2b9e5/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 889kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 329M/329M [00:03<00:00, 86.5MB/s] \n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 25.1kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 59.2kB/s]\n",
      "Downloading (…)2b9e5/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 2.16MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 1.12k/1.12k [00:00<00:00, 254kB/s]\n",
      "Downloading (…)c1ed2b9e5/vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 1.81MB/s]\n",
      "Downloading (…)ed2b9e5/modules.json: 100%|██████████| 229/229 [00:00<00:00, 40.5kB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Amanda <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">baked cookies</span> Do you you want some Jerry Sure <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Amanda ll bring</span> you tomorrow\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Amanda \u001b[30;48;2;255;255;0mbaked cookies\u001b[0m Do you you want some Jerry Sure \u001b[30;48;2;255;255;0mAmanda ll bring\u001b[0m you tomorrow\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baked  cookies', \"'ll bring you tomorrow\"]\n",
      "[('amanda ll bring', 0.2079), ('want jerry sure amanda', 0.2348), ('ll bring tomorrow', 0.3243), ('cookies', 0.3588), ('baked cookies', 0.4457)]\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "# hyperparameters\n",
    "keyphrase_ngram_range = (1, 4)\n",
    "maxsum_args = {'use_maxsum': True, 'nr_candidates': 20, 'top_n': 5} # `MaxSumSimilarity` with `nr_candidates` $< 20%$ of total number of unique words\n",
    "mmr_args = {'use_mmr': True, 'diversity': 0.7}\n",
    "model = 'paraphrase-distilroberta-base-v2' # \"all-MiniLM-L6-v2\" # stick to sentence transformers models as they are optimized, also the models using cosine similarity\n",
    "# 'msmarco-MiniLM-L6-cos-v5', 'msmarco-MiniLM-L12-cos-v5', 'msmarco-distilbert-cos-v5', 'allenai-specter', 'paraphrase-distilroberta-base-v2'\n",
    "shared_args = {'keyphrase_ngram_range': keyphrase_ngram_range, 'stop_words': 'english',}\n",
    "\n",
    "# load data\n",
    "with open(os.path.join(config.ENV_PATH, config.DATA_PATH, str('keybert_token_docs_' + \"train\" + \".txt\")),'rb') as f:\n",
    "    train_docs = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(config.ENV_PATH, config.DATA_PATH, str('keybert_token_keywords_' + \"train\" + \".txt\")), 'rb') as f:\n",
    "    train_keywords = pickle.load(f)\n",
    "\n",
    "# for testing\n",
    "doc = train_docs[0]\n",
    "keywords = train_keywords[0]\n",
    "\n",
    "# model\n",
    "kw_model = KeyBERT(model=model)\n",
    "\n",
    "# prepare embeddings to save time when changeing hyperparameters\n",
    "# use docs instead of doc in production\n",
    "doc_embeddings, word_embeddings = kw_model.extract_embeddings(doc,\n",
    "                                                              **shared_args)\n",
    "                                                            #   keyphrase_ngram_range=keyphrase_ngram_range,  \n",
    "                                                            #   stop_words='english')\n",
    "\n",
    "keywords_candidates = kw_model.extract_keywords(doc,\n",
    "                                                doc_embeddings=doc_embeddings, word_embeddings=word_embeddings,\n",
    "                                                highlight=True,\n",
    "                                                **shared_args,\n",
    "                                                **maxsum_args,)\n",
    "\n",
    "print(keywords)\n",
    "print(keywords_candidates)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: The model is of a form such that the phrase `'ll bring you tomorrow` is not even part of the candidates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models on the new data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MemSum\n",
    "\n",
    "Note:\n",
    "1. you need to switch to the folder src/MemSum_Full;\n",
    "2. You can specify the path to training and validation set, the model_folder (where you want to store model checkpoints) and the log_folder (where you want to store the log info), and other parameters. \n",
    "3. You can provide the absolute path, or relative path, as shown in the example code below.\n",
    "4. n_device means the number of available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src/MemSum_Full; python train.py -training_corpus_file_name ../../data/memsum_train_labelled.jsonl -validation_corpus_file_name ../../data/custom_data/val_CUSTOM_raw.jsonl -model_folder ../../model/MemSum_Full/custom_data/200dim/run0/ -log_folder ../../log/MemSum_Full/custom_data/200dim/run0/ -vocabulary_file_name ../../model/glove/vocabulary_200dim.pkl -pretrained_unigram_embeddings_file_name ../../model/glove/unigram_embeddings_200dim.pkl -max_seq_len 100 -max_doc_len 500 -num_of_epochs 10 -save_every 1000 -n_device 1 -batch_size_per_device 4 -max_extracted_sentences_per_document 7 -moving_average_decay 0.999 -p_stop_thres 0.6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the model hyperparameters, if applicable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MemSum\n",
    "\n",
    "* num_heads: (int, default = 8\n",
    "* hidden_dim: (int, default = 1024\n",
    "* N_enc_l: (int, default = 3\n",
    "* N_enc_g: (int, default = 3\n",
    "* N_dec: (int, default = 3\n",
    "* max_seq_len: (int, default = 100\n",
    "* max_doc_len: (int, default = 50\n",
    "* num_of_epochs: (int, default = 50\n",
    "* print_every: (int, default = 100\n",
    "* save_every: (int, default = 500\n",
    "* validate_every:  (int, default= 1000\n",
    "* restore_old_checkpoint: (bool, default = True)\n",
    "* learning_rate: (float, default = 1e-4\n",
    "* warmup_step:  (int, default= 1000\n",
    "* weight_decay: (float, default = 1e-6)\n",
    "* dropout_rate: (float, default = 0.1)\n",
    "* n_device: (int, default = 8)\n",
    "* batch_size_per_device: (int, default = 16)\n",
    "* max_extracted_sentences_per_document: (int)\n",
    "* moving_average_decay: (float)\n",
    "* p_stop_thres: (float, default = 0.7\n",
    "* apply_length_normalization: (int, default = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SciBERTSUM\n",
    "\n",
    "* attention mechanism\n",
    "* paper: learning rate is one of the most important hyper-parameters to tune"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keyBERT\n",
    "\n",
    "* different embeddings\n",
    "* number of keywords extracted, n-gram range (for keyphrases)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the models\n",
    "\n",
    "* what is the length of the summaries the models produce?\n",
    "* for human experts three criteria: non-redundancy, coverage, and overall quality\n",
    "* model runtimes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "* Nianlong Gu, Elliott Ash, and Richard Hahnloser. 2022. [MemSum: Extractive Summarization of Long Documents Using Multi-Step Episodic Markov Decision Processes.](https://aclanthology.org/2022.acl-long.450) In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6507–6522, Dublin, Ireland. Association for Computational Linguistics.\n",
    "* Sefid, Athar, and C. Lee Giles. [SciBERTSUM: Extractive Summarization for Scientific Documents.](https://doi.org/10.1007/978-3-031-06555-2_46) Document Analysis Systems: 15th IAPR International Workshop, DAS 2022, La Rochelle, France, May 22–25, 2022, Proceedings. Cham: Springer International Publishing, 2022.\n",
    "* Grootendorst, Maarten. [KeyBERT: Minimal keyword extraction with BERT.](https://doi.org/10.5281/zenodo.4461265) Zenodo (2020)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

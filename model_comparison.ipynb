{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximize and compare the accuracy of the following extractive summarization models: keyBERT, SciBERTSUM, MemSum\n",
    "\n",
    "Project work to complete the exam in [Text Mining by Prof. Gianluca Moro in the AC 2022/23 in Bologna](https://www.unibo.it/en/teaching/course-unit-catalogue/course-unit/2022/446610).\n",
    "\n",
    "The dataset is given and already divided into train, test and validation. The models have two targets to predict: relevant sentences and the relevant tokens. The accuracy on both targets should be maximized. The models to be used are:\n",
    "\n",
    "* [MemSum](https://github.com/nianlonggu/memsum)\n",
    "* [SciBERTSUM](https://github.com/atharsefid/SciBERTSUM)\n",
    "* [keyBERT](https://github.com/MaartenGr/KeyBERT)\n",
    "  * Simply the relevant sentences to be extracted will be those containing at least k relevant tokens, with k = 1, 2 or 3 that is an hyper parameter to be tuned."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project should be developed in colab with code commented in such a way that every step is understandable even without your verbal explanation in the discussion session."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task description\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "* ROUGE-N for MemSum [see](https://towardsdatascience.com/the-ultimate-performance-metric-in-nlp-111df6c64460)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-step Episodic Markov decision process extractive SUMmarizer (MemSum)\n",
    "\n",
    "Based on reinforcement-learning episodic Markov decision processes, this extractive summarizer uses at each step\n",
    "\n",
    "1. the text context of the sentence\n",
    "2. the global text context (!)\n",
    "3. and the extraction history (!)\n",
    "\n",
    "While iteratively selecting further sentences to extract, it also auto-selects the stop state. It produces concise summaries with little redudancy.\n",
    "\n",
    "Even though that the model is lightweight, it has SOTA performance on long documents from PubMed, arXiv, and GovReport\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SciBERTSUM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keyBERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Model | Technology | performance | context | history\n",
    "---|---|---|---|---\n",
    "MemSum | Markov decision processes | xy | local, global | extraction "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected results\n",
    "\n",
    "* MemSum: should be very good for long texts, short summaries compared to other models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "[Recommendations how to install packages](https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m venv env/ # only if you don't have a virtual environment\n",
    "!source env/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /usr/local/lib/python3.8/dist-packages\n",
      "sysconfig: /usr/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[33mWARNING: Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /usr/local/lib/python3.8/dist-packages\n",
      "sysconfig: /usr/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /usr/local/include/python3.8/UNKNOWN\n",
      "sysconfig: /usr/include/python3.8\u001b[0m\n",
      "\u001b[33mWARNING: Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /usr/local/bin\n",
      "sysconfig: /usr/bin\u001b[0m\n",
      "\u001b[33mWARNING: Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /usr/local\n",
      "sysconfig: /usr\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://download.pytorch.org/whl/cu116/torch_stable.html\n",
      "Requirement already satisfied: numpy in /home/work/.local/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.22.4)\n",
      "Requirement already satisfied: pandas in /home/work/.local/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.5.1)\n",
      "Requirement already satisfied: torchsummary in /home/work/.local/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (1.5.1)\n",
      "Requirement already satisfied: tqdm in /home/work/.local/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (4.60.0)\n",
      "Requirement already satisfied: rouge_score in /home/work/.local/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (0.1.2)\n",
      "Requirement already satisfied: pyrouge in /home/work/.local/lib/python3.8/site-packages (from -r requirements.txt (line 12)) (0.1.3)\n",
      "Requirement already satisfied: matplotlib in /home/work/.local/lib/python3.8/site-packages (from -r requirements.txt (line 15)) (3.4.3)\n",
      "Requirement already satisfied: seaborn in /home/work/.local/lib/python3.8/site-packages (from -r requirements.txt (line 16)) (0.11.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/work/.local/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 3)) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/work/.local/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: absl-py in /home/work/.local/lib/python3.8/site-packages (from rouge_score->-r requirements.txt (line 11)) (0.12.0)\n",
      "Requirement already satisfied: nltk in /home/work/.local/lib/python3.8/site-packages (from rouge_score->-r requirements.txt (line 11)) (3.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/work/.local/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 15)) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/work/.local/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 15)) (8.3.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/work/.local/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 15)) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 15)) (2.4.7)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/work/.local/lib/python3.8/site-packages (from seaborn->-r requirements.txt (line 16)) (1.7.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/work/.local/lib/python3.8/site-packages (from nltk->rouge_score->-r requirements.txt (line 11)) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /home/work/.local/lib/python3.8/site-packages (from nltk->rouge_score->-r requirements.txt (line 11)) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->rouge_score->-r requirements.txt (line 11)) (7.1.2)\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /home/work/.local/include/python3.8/UNKNOWN\n",
      "sysconfig: /home/work/.local/include/python3.8\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = True\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import config \n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the extractive summarization models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MemSum\n",
    "\n",
    "The model expects:\n",
    "\n",
    "- [x] The training data is now stored in a .jsonl file that contains a list of json info, one line for one training instance. Each json (or dictonary) contains two keys:\n",
    "  1. \"text\": the value for which is a python list of sentences, this represents the document you want to summarize;\n",
    "  2. \"summary\": the value is also a list of sentences. If represent the ground-truth summary. Because the summary can contain multiple sentences, so we store them as a list.\n",
    "- [x] high-ROUGE episodes for the training set\n",
    "- [x] download glove into the `model/glove/` folder\n",
    "\n",
    "- [ ] we need to furthermore make a dataset with all the tokes instead of sentences as the basic thing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/work/Dokumente/Studium/SimTech_MSc/Erasmus/Lectures/Text_mining/project/ExtractiveSummaryModels/MemSum'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(MemSum)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing custom data\n",
    "with open(os.path.join(config.ENV_PATH, config.DATA_PATH, config.name_trainset + '.json')) as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "memsum_training = os.path.join(config.ENV_PATH, config.DATA_PATH, str('memsum_' + config.name_trainset + \".jsonl\"))\n",
    "with open(memsum_training, 'w') as outfile:\n",
    "    for l in json_data:\n",
    "        dialog_temp = {}\n",
    "\n",
    "        # the dialogue is the text\n",
    "        text_temp = []\n",
    "        sentences = l['dialogue'].split('#')\n",
    "        for s in sentences:\n",
    "            # remove empty strings\n",
    "            # to normalize between train and test, we need to remove the empty end of the sentence\n",
    "            s = s.strip('\\n') # TODO: this accidently also strips stuff like ')'\n",
    "            if s == '':\n",
    "                continue\n",
    "            else:\n",
    "                if s[-1] == ' ': # TODO: this does not work as expected\n",
    "                    text_temp.append(s[:-1])\n",
    "                else:\n",
    "                    text_temp.append(s)\n",
    "        dialog_temp['text'] = text_temp\n",
    "        \n",
    "        # the relevant dialogue is the summary\n",
    "        dialog_temp['summary'] = [s.strip('#') for s in l['relevant_dialogue']] # remove the # because it is also removed from the training\n",
    "\n",
    "        # # check if sth goes wrong\n",
    "        # for s in dialog_temp['summary']:\n",
    "        #     if s not in dialog_temp['text']:\n",
    "        #         print(f\"The summary of the sentence '{s}' is not in text {dialog_temp['text']}\")\n",
    "\n",
    "        # dialog_temp['finegrained_relevant_dialogue']: I don't think this model can work with this\n",
    "\n",
    "        outfile.write(json.dumps(dialog_temp)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14729\n",
      "dict_keys(['text', 'summary'])\n",
      "[' Amanda: I baked  cookies. Do you want some?', ' Jerry: Sure!', \" Amanda: I'll bring you tomorrow :-)\"]\n",
      "[' Amanda: I baked  cookies. Do you want some?', \" Amanda: I'll bring you tomorrow :-)\"]\n"
     ]
    }
   ],
   "source": [
    "# check that everything is fine\n",
    "train_corpus = [ json.loads(line) for line in open(os.path.join(config.ENV_PATH, config.DATA_PATH, str('memsum_' + config.name_trainset + \".jsonl\"))) ]\n",
    "\n",
    "## as an example, we have 100 instances for training\n",
    "print(len(train_corpus))\n",
    "print(train_corpus[0].keys())\n",
    "print(train_corpus[0][\"text\"][:3])\n",
    "print(train_corpus[0][\"summary\"][:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data/\n",
    "! cp ../data/memsum_train.json data/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] repeat for not-training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 3895/14729 [03:51<10:44, 16.81it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_76973/2970157870.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/memsum_train.jsonl\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mhigh_rouge_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgreedy_extract\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"summary\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeamsearch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mindices_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mscore_list\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dokumente/Studium/SimTech_MSc/Erasmus/Lectures/Text_mining/project/ExtractiveSummaryModels/MemSum/src/data_preprocessing/MemSum/utils.py\u001b[0m in \u001b[0;36mgreedy_extract\u001b[0;34m(document, summary, beamsearch_size, max_num_extracted_sentences, max_num_extractions, epsilon, n_gram_list, metric)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mmax_num_extracted_sentences\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mgreedy_extract_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin_items\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0msummary_seq\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mextracted_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeamsearch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_extracted_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_extractions\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mcandidate_extractions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gram_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_gram_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0mcandidate_extractions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dokumente/Studium/SimTech_MSc/Erasmus/Lectures/Text_mining/project/ExtractiveSummaryModels/MemSum/src/data_preprocessing/MemSum/utils.py\u001b[0m in \u001b[0;36mgreedy_extract_core\u001b[0;34m(document, summary, extracted_indices, beamsearch_size, max_num_extracted_sentences, max_num_extractions, candidate_extractions, epsilon, n_gram_list, metric)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgain_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mgreedy_extract_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextracted_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeamsearch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_num_extracted_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_extractions\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcandidate_extractions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gram_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_gram_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dokumente/Studium/SimTech_MSc/Erasmus/Lectures/Text_mining/project/ExtractiveSummaryModels/MemSum/src/data_preprocessing/MemSum/utils.py\u001b[0m in \u001b[0;36mgreedy_extract_core\u001b[0;34m(document, summary, extracted_indices, beamsearch_size, max_num_extracted_sentences, max_num_extractions, candidate_extractions, epsilon, n_gram_list, metric)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgain_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mgreedy_extract_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextracted_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeamsearch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_num_extracted_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_extractions\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcandidate_extractions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gram_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_gram_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dokumente/Studium/SimTech_MSc/Erasmus/Lectures/Text_mining/project/ExtractiveSummaryModels/MemSum/src/data_preprocessing/MemSum/utils.py\u001b[0m in \u001b[0;36mgreedy_extract_core\u001b[0;34m(document, summary, extracted_indices, beamsearch_size, max_num_extracted_sentences, max_num_extractions, candidate_extractions, epsilon, n_gram_list, metric)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgain_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mgreedy_extract_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextracted_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeamsearch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_num_extracted_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_extractions\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcandidate_extractions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gram_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_gram_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dokumente/Studium/SimTech_MSc/Erasmus/Lectures/Text_mining/project/ExtractiveSummaryModels/MemSum/src/data_preprocessing/MemSum/utils.py\u001b[0m in \u001b[0;36mgreedy_extract_core\u001b[0;34m(document, summary, extracted_indices, beamsearch_size, max_num_extracted_sentences, max_num_extractions, candidate_extractions, epsilon, n_gram_list, metric)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgain_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mgreedy_extract_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextracted_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeamsearch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_num_extracted_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_extractions\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcandidate_extractions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gram_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_gram_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dokumente/Studium/SimTech_MSc/Erasmus/Lectures/Text_mining/project/ExtractiveSummaryModels/MemSum/src/data_preprocessing/MemSum/utils.py\u001b[0m in \u001b[0;36mgreedy_extract_core\u001b[0;34m(document, summary, extracted_indices, beamsearch_size, max_num_extracted_sentences, max_num_extractions, candidate_extractions, epsilon, n_gram_list, metric)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgain_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mgreedy_extract_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextracted_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeamsearch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_num_extracted_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_extractions\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcandidate_extractions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gram_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_gram_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dokumente/Studium/SimTech_MSc/Erasmus/Lectures/Text_mining/project/ExtractiveSummaryModels/MemSum/src/data_preprocessing/MemSum/utils.py\u001b[0m in \u001b[0;36mgreedy_extract_core\u001b[0;34m(document, summary, extracted_indices, beamsearch_size, max_num_extracted_sentences, max_num_extractions, candidate_extractions, epsilon, n_gram_list, metric)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgain_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mgreedy_extract_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextracted_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeamsearch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_num_extracted_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_extractions\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcandidate_extractions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gram_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_gram_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dokumente/Studium/SimTech_MSc/Erasmus/Lectures/Text_mining/project/ExtractiveSummaryModels/MemSum/src/data_preprocessing/MemSum/utils.py\u001b[0m in \u001b[0;36mgreedy_extract_core\u001b[0;34m(document, summary, extracted_indices, beamsearch_size, max_num_extracted_sentences, max_num_extractions, candidate_extractions, epsilon, n_gram_list, metric)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0msummary_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mremaining_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mnew_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gram_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_gram_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0mnew_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnew_scores\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dokumente/Studium/SimTech_MSc/Erasmus/Lectures/Text_mining/project/ExtractiveSummaryModels/MemSum/src/data_preprocessing/MemSum/utils.py\u001b[0m in \u001b[0;36mget_score\u001b[0;34m(hyp, ref, n_gram_list, history_results, metric)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mhyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gram_list\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"fmeasure\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfast_rouge_score\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gram_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_results\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rouge%d\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mn_gram_list\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dokumente/Studium/SimTech_MSc/Erasmus/Lectures/Text_mining/project/ExtractiveSummaryModels/MemSum/src/data_preprocessing/MemSum/utils.py\u001b[0m in \u001b[0;36mfast_rouge_score\u001b[0;34m(ref, hyp, n_gram_list, history_results)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0munique_ref_ngram_expanded_for_hyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_ref_ngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mhyp_ngram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mn_match_in_hyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_ref_ngram_expanded_for_hyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mhyp_ngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mn_match_in_hyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_match_in_hyp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhistory_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rouge%d\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_match_in_hyp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mn_hyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyp_ngram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhistory_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rouge%d\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_hyp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mall\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mall\u001b[0;34m(a, axis, out, keepdims, where)\u001b[0m\n\u001b[1;32m   2485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2486\u001b[0m     \"\"\"\n\u001b[0;32m-> 2487\u001b[0;31m     return _wrapreduction(a, np.logical_and, 'all', axis, None, out,\n\u001b[0m\u001b[1;32m   2488\u001b[0m                           keepdims=keepdims, where=where)\n\u001b[1;32m   2489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.data_preprocessing.MemSum.utils import greedy_extract\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_corpus = [ json.loads(line) for line in open(\"data/memsum_train.jsonl\") ]\n",
    "for data in tqdm(train_corpus):\n",
    "    high_rouge_episodes = greedy_extract( data[\"text\"], data[\"summary\"], beamsearch_size = 2\n",
    "    indices_list = []\n",
    "    score_list  = []\n",
    "\n",
    "    for indices, score in high_rouge_episodes:\n",
    "        indices_list.append( indices )\n",
    "        score_list.append(score)\n",
    "\n",
    "    data[\"indices\"] = indices_list\n",
    "    data[\"score\"] = score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/memsum_train_labelled.jsonl\",\"w\") as f:\n",
    "    for data in train_corpus:\n",
    "        f.write(json.dumps(data) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models on the new data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MemSum\n",
    "\n",
    "Note:\n",
    "1. you need to switch to the folder src/MemSum_Full;\n",
    "2. You can specify the path to training and validation set, the model_folder (where you want to store model checkpoints) and the log_folder (where you want to store the log info), and other parameters. \n",
    "3. You can provide the absolute path, or relative path, as shown in the example code below.\n",
    "4. n_device means the number of available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src/MemSum_Full; python train.py -training_corpus_file_name ../../data/memsum_train_labelled.jsonl -validation_corpus_file_name ../../data/custom_data/val_CUSTOM_raw.jsonl -model_folder ../../model/MemSum_Full/custom_data/200dim/run0/ -log_folder ../../log/MemSum_Full/custom_data/200dim/run0/ -vocabulary_file_name ../../model/glove/vocabulary_200dim.pkl -pretrained_unigram_embeddings_file_name ../../model/glove/unigram_embeddings_200dim.pkl -max_seq_len 100 -max_doc_len 500 -num_of_epochs 10 -save_every 1000 -n_device 1 -batch_size_per_device 4 -max_extracted_sentences_per_document 7 -moving_average_decay 0.999 -p_stop_thres 0.6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the model hyperparameters, if applicable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MemSum\n",
    "\n",
    "* num_heads: (int, default = 8\n",
    "* hidden_dim: (int, default = 1024\n",
    "* N_enc_l: (int, default = 3\n",
    "* N_enc_g: (int, default = 3\n",
    "* N_dec: (int, default = 3\n",
    "* max_seq_len: (int, default = 100\n",
    "* max_doc_len: (int, default = 50\n",
    "* num_of_epochs: (int, default = 50\n",
    "* print_every: (int, default = 100\n",
    "* save_every: (int, default = 500\n",
    "* validate_every:  (int, default= 1000\n",
    "* restore_old_checkpoint: (bool, default = True)\n",
    "* learning_rate: (float, default = 1e-4\n",
    "* warmup_step:  (int, default= 1000\n",
    "* weight_decay: (float, default = 1e-6)\n",
    "* dropout_rate: (float, default = 0.1)\n",
    "* n_device: (int, default = 8)\n",
    "* batch_size_per_device: (int, default = 16)\n",
    "* max_extracted_sentences_per_document: (int)\n",
    "* moving_average_decay: (float)\n",
    "* p_stop_thres: (float, default = 0.7\n",
    "* apply_length_normalization: (int, default = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the models\n",
    "\n",
    "* what is the length of the summaries the models produce?\n",
    "* for human experts three criteria: non-redundancy, coverage, and overall quality\n",
    "* model runtimes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "* Nianlong Gu, Elliott Ash, and Richard Hahnloser. 2022. [MemSum: Extractive Summarization of Long Documents Using Multi-Step Episodic Markov Decision Processes.](https://aclanthology.org/2022.acl-long.450) In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6507–6522, Dublin, Ireland. Association for Computational Linguistics.\n",
    "* Sefid, Athar, and C. Lee Giles. [SciBERTSUM: Extractive Summarization for Scientific Documents.](https://doi.org/10.1007/978-3-031-06555-2_46) Document Analysis Systems: 15th IAPR International Workshop, DAS 2022, La Rochelle, France, May 22–25, 2022, Proceedings. Cham: Springer International Publishing, 2022.\n",
    "* Grootendorst, Maarten. [KeyBERT: Minimal keyword extraction with BERT.](https://doi.org/10.5281/zenodo.4461265) Zenodo (2020)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

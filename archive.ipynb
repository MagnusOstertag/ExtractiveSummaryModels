{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especially interesting are those three dialoges where the number of tokens in the finegrained relevant dialog is greater than in the relevant dialog. We look at them now, but this seems wrong and we might have to exclude them later on: \n",
    "\n",
    "- [ ] there is either an error in the calculation or an error here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_samples = metadata[metadata['n_tokens_per_extract_fine'] > metadata['n_tokens_per_extract_rel']]\n",
    "\n",
    "print(f\"The index and the dataset of them is \\n {problematic_samples['name']}\")\n",
    "\n",
    "# index of the problematic samples\n",
    "problematic_samples_idx = problematic_samples.index.to_list()\n",
    "offset = metadata[metadata['name'] == 'test'].count()['name']\n",
    "print(f\"The offset is {offset}\")\n",
    "problematic_samples_idx = problematic_samples_idx - offset\n",
    "\n",
    "with open(os.path.join(config.ENV_PATH, config.DATA_PATH, 'train.json'), 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "for i, l in enumerate(json_data):\n",
    "    if i in problematic_samples_idx:\n",
    "        print(f\"Sample {i} has the relevant \\n {l['relevant_dialogue']} \\n and the finegraned relevant: \\n {l['finegrained_relevant_dialogue']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(config.ENV_PATH, config.DATA_PATH, str('keybert_token_keywords_' + \"train\" + \".txt\")), 'rb') as f:\n",
    "    train_keywords = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(config.ENV_PATH, config.DATA_PATH, str('keybert_token_keywords_' + \"test\" + \".txt\")), 'rb') as f:\n",
    "    test_keywords = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(config.ENV_PATH, config.DATA_PATH, str('keybert_token_keywords_' + \"val\" + \".txt\")), 'rb') as f:\n",
    "    val_keywords = pickle.load(f)\n",
    "\n",
    "longest_ngram_train = 0\n",
    "longest_ngram_train_text = ''\n",
    "for i in train_keywords:\n",
    "    for j in i:\n",
    "        if len(j.split(' ')) > longest_ngram_train:\n",
    "            longest_ngram_train = len(j.split(' '))\n",
    "            longest_ngram_train_text = j\n",
    "print(longest_ngram_train_text)\n",
    "print(longest_ngram_train, '\\n')\n",
    "\n",
    "longest_ngram_test = 0\n",
    "longest_ngram_test_text = ''\n",
    "for i in test_keywords:\n",
    "    for j in i:\n",
    "        if len(j.split(' ')) > longest_ngram_test:\n",
    "            longest_ngram_test = len(j.split(' '))\n",
    "            longest_ngram_test_text = j\n",
    "print(longest_ngram_test_text)\n",
    "print(longest_ngram_test, '\\n')\n",
    "\n",
    "longest_ngram_val = 0\n",
    "longest_ngram_val_text = ''\n",
    "for i in val_keywords:\n",
    "    for j in i:\n",
    "        if len(j.split(' ')) > longest_ngram_val:\n",
    "            longest_ngram_val = len(j.split(' '))\n",
    "            longest_ngram_val_text = j\n",
    "print(longest_ngram_val_text)\n",
    "print(longest_ngram_val, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "267b10c2854b83cc19e2a82e3965dcaad84fa4fdc43e42edc62ac2d19787cd67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
